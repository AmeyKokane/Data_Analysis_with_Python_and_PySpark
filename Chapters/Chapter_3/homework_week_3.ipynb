{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "1. When must you use spark-submit to run your Spark program?\n",
    "* `spark-submit` is used to run Spark program in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercises:\n",
    "\n",
    "# 1.    Grab the complete works of William Shakespeare. Create a program that calculates all word frequencies. \n",
    "#       How many times does the word \"woe\" appear? How many distinct words are there?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/05 23:11:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice\")\n",
    "         .master(\"local[8]\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/05 23:12:12 WARN SparkContext: The path https://www.gutenberg.org/cache/epub/100/pg100.txt has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|word_clean|count|\n",
      "+----------+-----+\n",
      "|       woe|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "#df = spark.read.csv(\"file://\"+SparkFiles.get(\"adult.csv\"), header=True, inferSchema= True)\n",
    "result = (spark.read.text(\"file://\"+SparkFiles.get(\"pg100.txt\"))\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "    .select(F.regexp_extract(F.col(\"word_lower\"), \"[a-z]*\",0).alias(\"word_clean\"))\n",
    "    .filter(F.col(\"word_clean\") == \"woe\")\n",
    "    .groupby(F.col(\"word_clean\"))\n",
    "    .count()\n",
    "    .show()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-pyspark332",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
