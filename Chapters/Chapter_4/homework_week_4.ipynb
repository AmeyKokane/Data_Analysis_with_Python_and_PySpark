{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1\n",
    " Let’s take the following file, called sample.csv, which contains three columns:\n",
    "\n",
    " Item,Quantity,Price\n",
    "\n",
    " Banana, organic,1,0.99\n",
    "\n",
    " Pear,7,1.24\n",
    "\n",
    " Cake, chocolate,1,14.50\n",
    "\n",
    " Complete the following code to ingest the file successfully.\n",
    " \n",
    "```python\n",
    "sample = spark.read.csv([...],\n",
    "                        sep=[...],\n",
    "                        header=[...],\n",
    "                        quote=[...],\n",
    "                        inferSchema=[...]\n",
    ")\n",
    "```\n",
    "(Note: If you want to test your code, sample.csv is available in the book’s repository under data/sample.csv/sample.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "csv_file_path = \"/Users/u354769/Desktop/Ameya_Learning/DataAnalysisWithPythonAndPySpark/Book_Materials/Chapters/Chapter_4/Data/sample1.csv\"\n",
    "sample = spark.read.csv(csv_file_path,\n",
    "                        sep=\",\",\n",
    "                        header=True,\n",
    "                        quote=\"$\",\n",
    "                        inferSchema=True)\n",
    "\n",
    "sample.show()\n",
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2\n",
    "\n",
    "What is the printed result of this code?\n",
    "\n",
    "sample_frame.columns # => ['item', 'price', 'quantity', 'UPC']\n",
    "\n",
    "print(sample_frame.drop('item', 'UPC', 'prices').columns)\n",
    "\n",
    "a ['item' 'UPC']\n",
    "\n",
    "b ['item', 'upc']\n",
    "\n",
    "c ['price', 'quantity'] <--- Answer\n",
    "\n",
    "d ['price', 'quantity', 'UPC']\n",
    "\n",
    "e Raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 4.3\n",
    "# Reread the data in a logs_raw data frame (the data file is ./data/broadcast_logs- BroadcastLogs_2018_Q3_M8.CSV), \n",
    "# this time without passing any optional parameters. Print the first five rows of data, as well as the schema. \n",
    "# What are the differences in terms of data and schema between logs and logs_raw?\n",
    "\n",
    "csv_file_path = \"/Users/u354769/Desktop/Ameya_Learning/DataAnalysisWithPythonAndPySpark/Book_Materials/Chapters/Chapter_4/Data/BroadcastLogs_2018_Q3_M8.csv\"\n",
    "logs_raw = spark.read.csv(csv_file_path,\n",
    "                        # sep=\",\",\n",
    "                        # header=True,\n",
    "                        # quote=\"$\",\n",
    "                        # inferSchema=True\n",
    "                        )\n",
    "logs_raw.show(5, False)\n",
    "\n",
    "##ANSWER:\n",
    "# Data for all columns crammed into one single column.\n",
    "# Header name is defaulted to _c0\n",
    "# timestamp format includes micro-seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 4.4\n",
    "# Create a new data frame, logs_clean, that contains only the columns that do not end with ID.\n",
    "\n",
    "logs = spark.read.csv(csv_file_path\n",
    "                        , sep = \"|\"\n",
    "                        , header = True\n",
    "                        , inferSchema = True\n",
    "                        , timestampFormat=\"yyyy-MM-dd\",\n",
    "                        )\n",
    "logs_clean = logs.select(\n",
    "    *[x for x in logs.columns if \"ID\" in x]\n",
    ").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/13 08:32:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                               |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                            |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                             |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                           |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Titanic dataset) into a dataframe. Filter the rows and columns in some manner, and write the result to a parquet file. \n",
    "# Read the parquet file back in to verify everything worked as you'd expect.\n",
    "\n",
    "\n",
    "\n",
    "csv_file_path = \"/Users/u354769/Desktop/Ameya_Learning/DataAnalysisWithPythonAndPySpark/Book_Materials/Chapters/Chapter_4/Data/titanic.csv\"\n",
    "\n",
    "\n",
    "titanic_df = spark.read.csv(csv_file_path\n",
    "                        , sep = \",\"\n",
    "                        , header = True\n",
    "                        , inferSchema = True\n",
    "                )\n",
    "titanic_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing as parquet file\n",
    "parquet_write_path = \"/Users/u354769/Desktop/Ameya_Learning/DataAnalysisWithPythonAndPySpark/Book_Materials/Chapters/Chapter_4/Data/titanic_df.parquet\"\n",
    "titanic_df.write.parquet(parquet_write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                               |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                            |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                             |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                           |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the titanic_df parquet file\n",
    "titanic_df_parquet = spark.read.parquet(parquet_write_path)\n",
    "titanic_df_parquet.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|summary|    Fare|\n",
      "+-------+--------+\n",
      "|     1%|     0.0|\n",
      "|     2%|  6.2375|\n",
      "|     3%|   6.975|\n",
      "|     4%|    7.05|\n",
      "|     5%|   7.225|\n",
      "|     6%|   7.225|\n",
      "|     7%|  7.2292|\n",
      "|     8%|    7.25|\n",
      "|     9%|    7.25|\n",
      "|    10%|    7.55|\n",
      "|    11%|  7.7292|\n",
      "|    12%|    7.75|\n",
      "|    13%|    7.75|\n",
      "|    14%|    7.75|\n",
      "|    15%|    7.75|\n",
      "|    16%|   7.775|\n",
      "|    17%|   7.775|\n",
      "|    18%|  7.7958|\n",
      "|    19%|  7.8542|\n",
      "|    20%|  7.8542|\n",
      "|    21%|  7.8958|\n",
      "|    22%|  7.8958|\n",
      "|    23%|  7.8958|\n",
      "|    24%|  7.8958|\n",
      "|    25%|  7.8958|\n",
      "|    26%|   7.925|\n",
      "|    27%|   7.925|\n",
      "|    28%|    8.05|\n",
      "|    29%|    8.05|\n",
      "|    30%|    8.05|\n",
      "|    31%|    8.05|\n",
      "|    32%|  8.1125|\n",
      "|    33%|  8.6542|\n",
      "|    34%|  8.6625|\n",
      "|    35%|     9.0|\n",
      "|    36%|     9.5|\n",
      "|    37%|  9.5875|\n",
      "|    38%| 10.4625|\n",
      "|    39%|    10.5|\n",
      "|    40%|    10.5|\n",
      "|    41%| 11.1333|\n",
      "|    42%|  12.275|\n",
      "|    43%|  12.525|\n",
      "|    44%|    13.0|\n",
      "|    45%|    13.0|\n",
      "|    46%|    13.0|\n",
      "|    47%|    13.0|\n",
      "|    48%|    13.0|\n",
      "|    49%|    14.0|\n",
      "|    50%| 14.4542|\n",
      "|    51%|    14.5|\n",
      "|    52%| 15.2458|\n",
      "|    53%|    15.5|\n",
      "|    54%|   15.85|\n",
      "|    55%|    16.1|\n",
      "|    56%|    17.8|\n",
      "|    57%| 19.2583|\n",
      "|    58%| 20.2125|\n",
      "|    59%|    21.0|\n",
      "|    60%| 21.6792|\n",
      "|    61%|   23.25|\n",
      "|    62%|   24.15|\n",
      "|    63%| 25.5875|\n",
      "|    64%|    26.0|\n",
      "|    65%|    26.0|\n",
      "|    66%|    26.0|\n",
      "|    67%|   26.25|\n",
      "|    68%| 26.2875|\n",
      "|    69%|   26.55|\n",
      "|    70%|    27.0|\n",
      "|    71%|   27.75|\n",
      "|    72%|    29.0|\n",
      "|    73%|    29.7|\n",
      "|    74%|    30.5|\n",
      "|    75%|    31.0|\n",
      "|    76%| 31.3875|\n",
      "|    77%| 34.0208|\n",
      "|    78%|    35.5|\n",
      "|    79%|    39.0|\n",
      "|    80%| 39.6875|\n",
      "|    81%|    46.9|\n",
      "|    82%|    50.0|\n",
      "|    83%|    52.0|\n",
      "|    84%|    53.1|\n",
      "|    85%| 56.4958|\n",
      "|    86%| 57.9792|\n",
      "|    87%|    66.6|\n",
      "|    88%|   69.55|\n",
      "|    89%|    73.5|\n",
      "|    90%| 77.9583|\n",
      "|    91%|    79.2|\n",
      "|    92%| 82.1708|\n",
      "|    93%| 89.1042|\n",
      "|    94%|    93.5|\n",
      "|    95%| 113.275|\n",
      "|    96%|   134.5|\n",
      "|    97%|  151.55|\n",
      "|    98%|211.3375|\n",
      "|    99%| 262.375|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a query that calculates the 1st-99th percentiles of the Fare column in the Titanic dataset.\n",
    "\n",
    "percentile_list =[\"1%\",\t\"2%\",\t\"3%\",\t\"4%\",\t\"5%\",\t\"6%\",\t\"7%\",\t\"8%\",\t\"9%\",\t\"10%\",\t\"11%\",\t\"12%\",\t\"13%\",\t\"14%\",\t\"15%\",\t\"16%\",\t\"17%\",\t\"18%\",\t\"19%\",\t\"20%\",\t\"21%\",\t\"22%\",\t\"23%\",\t\"24%\",\t\"25%\",\t\"26%\",\t\"27%\",\t\"28%\",\t\"29%\",\t\"30%\",\t\"31%\",\t\"32%\",\t\"33%\",\t\"34%\",\t\"35%\",\t\"36%\",\t\"37%\",\t\"38%\",\t\"39%\",\t\"40%\",\t\"41%\",\t\"42%\",\t\"43%\",\t\"44%\",\t\"45%\",\t\"46%\",\t\"47%\",\t\"48%\",\t\"49%\",\t\"50%\",\t\"51%\",\t\"52%\",\t\"53%\",\t\"54%\",\t\"55%\",\t\"56%\",\t\"57%\",\t\"58%\",\t\"59%\",\t\"60%\",\t\"61%\",\t\"62%\",\t\"63%\",\t\"64%\",\t\"65%\",\t\"66%\",\t\"67%\",\t\"68%\",\t\"69%\",\t\"70%\",\t\"71%\",\t\"72%\",\t\"73%\",\t\"74%\",\t\"75%\",\t\"76%\",\t\"77%\",\t\"78%\",\t\"79%\",\t\"80%\",\t\"81%\",\t\"82%\",\t\"83%\",\t\"84%\",\t\"85%\",\t\"86%\",\t\"87%\",\t\"88%\",\t\"89%\",\t\"90%\",\t\"91%\",\t\"92%\",\t\"93%\",\t\"94%\",\t\"95%\",\t\"96%\",\t\"97%\",\t\"98%\",\t\"99%\"]\n",
    "titanic_df.select(\"Fare\").summary(percentile_list).show(99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-pyspark332",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
